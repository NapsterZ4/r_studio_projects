---
title: "Exámen final de predicción, ejercicio dificil"
date: "`r Sys.Date()`"
author: "Jorge Emilio Zapata Godoy"
output:
  rmdformats::readthedown:
    highlight: kate
---


```{r setup, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=FALSE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

# Cargando librerías
```{r Librerias}
library(ggplot2)
library(factoextra)
library(dendextend)
library(DT)
library(d3heatmap)
library(igraph)
library(tidygraph)
library(ggpubr)
library(gridExtra)
library(boot)
library(caTools)
library(ROCR)
library(e1071)
library(randomForest)
library(rpart)
library(caret)
library(reshape)
library(plotly)
library(tidyverse)
library(dplyr)
library(tidyr)
library(purrr)
library(recipes)
library(doParallel)
library(klaR)
library(C50)
library(ranger)
library(kernlab)
library(modelgrid)
library(corrplot)
```


# Inicializando datos
```{r Inicializando datos}
setwd("/mnt/napster_disk/LEAD University/IQ - 2020/Minería de Datos Avanzada/examen_predictivo/")
data <- read.csv("datos.corazon.train.csv", sep = ",", header = TRUE)
data
```

# Análisis exploratorio

## Tipos de datos
Cambiamos la tipología de los datos de todas las variables necesarias que son binarias o tienen entre 0 - 3 valores nulos tipo factor
```{r Tipos de datos}
glimpse(data)
data <- data[,c(2,3,4,5,6,7,8,9,10,11,12,13,14,15)]

data$sex <- if_else(data$sex == 1, "M", "F")
data$target <- if_else(data$target == 1, "si", "no")

data$sex <- as.factor(data$sex)
data$target <- as.factor(data$target)
data$cp <- as.factor(data$cp)
data$fbs <- as.factor(data$fbs)
data$restecg <- as.factor(data$fbs)
data$exang <- as.factor(data$exang)
data$slope <- as.factor(data$slope)
data$ca <- as.factor(data$ca)
data$thal <- as.factor(data$thal)

datatable(data = data)
```

## Cantidad de nulos
No cuenta con datos ausentes o datos nulos.
```{r Cantidad de nulos}
any(!complete.cases(data))
map_dbl(data, .f = function(x){sum(is.na(x))})
```

## Distribución de variables
La variable respuesta cuenta con 150 datos aproximadamente en punto 1. No logramos observar datos con mucha dispersión y outliers. La variable `talach` tiene valores que van entre 0 - 200 y la variable `chol` con valores entre 0 - 500, por lo tanto trataremos de normalizarlos junto con el resto (aplicar una transformación).
```{r Distribución de variables}
plot1 <- ggplot(data = data, aes(x = target, y = age, color = target)) + geom_boxplot(outlier.colour = "green") + geom_jitter() + theme_bw()
plot2 <- ggplot(data = data, aes(x = target, y = sex, color = target)) + geom_boxplot(outlier.color = "green") + geom_jitter() + theme_bw()
plot3 <- ggplot(data = data, aes(x = target, y = cp, color = target)) + geom_boxplot(outlier.color = "green") + geom_jitter() + theme_bw()
plot4 <- ggplot(data = data, aes(x = target, y = trestbps, color = target)) + geom_boxplot(outlier.color = "green") + geom_jitter() + theme_bw()
plot5 <- ggplot(data = data, aes(x = target, y = chol, color = target)) + geom_boxplot(outlier.color = "green") + geom_jitter() + theme_bw()
plot6 <- ggplot(data = data, aes(x = target, y = fbs, color = target)) + geom_boxplot(outlier.color = "green") + geom_jitter() + theme_bw()
plot7 <- ggplot(data = data, aes(x = target, y = restecg, color = target)) + geom_boxplot(outlier.color = "green") + geom_jitter() + theme_bw()
plot8 <- ggplot(data = data, aes(x = target, y = thalach, color = target)) + geom_boxplot(outlier.color = "green") + geom_jitter() + theme_bw()
plot9 <- ggplot(data = data, aes(x = target, y = exang, color = target)) + geom_boxplot(outlier.color = "green") + geom_jitter() + theme_bw()
plot10 <- ggplot(data = data, aes(x = target, y = oldpeak, color = target)) + geom_boxplot(outlier.color = "green") + geom_jitter() + theme_bw()
plot11 <- ggplot(data = data, aes(x = target, y = slope, color = target)) + geom_boxplot(outlier.color = "green") + geom_jitter() + theme_bw()
plot12 <- ggplot(data = data, aes(x = target, y = ca, color = target)) + geom_boxplot(outlier.color = "green") + geom_jitter() + theme_bw()
plot13 <- ggplot(data = data, aes(x = target, y = thal, color = target)) + geom_boxplot(outlier.color = "green") + geom_jitter() + theme_bw()

final.plot1 <- ggarrange(plot1, plot2, plot3, plot4, plot5, plot6, plot7, plot8, plot9, ncol = 3, legend = "top")
final.plot2 <- ggarrange(plot10, plot11, plot12, plot13, ncol = 2, legend = "top")

final.plot1
final.plot2
```

## Correlación de variables continuas
Vemos la correlación existente entre variables continuas y variables numéricas discretas. Las variables `trestbps`, `chol` y `thalach` tienen una mayor correlación, esto nos indica cuales variables debemos tomar en cuenta para analizar.
```{r Correlacion de variables continuas}
correlations <- cor(select_if(data, is.numeric), method = "pearson")
corrplot(correlations, method = "circle")
```

## Random forest para seleccionar variables
Esta técnica funciona bien cuando no se tienen valores faltantes, por lo tanto se procede a utilizarla. Observamos que `ca`, `cp`, `thal` como se encuentran correlacionadas, también se muestran como mejores predictores.
```{r Random Forest para seleccionar variables}
data.rf <- data %>% dplyr::select(-age) %>% na.omit()
data.rf <- map_if(.x = data, .p = is.character, .f = as.factor) %>% as.data.frame()

model.rf <- randomForest(formula = target ~., data = na.omit(data.rf), mtry = 5, importante = TRUE, ntree = 1000)
importance <- as.data.frame(model.rf$importance)
importance <- rownames_to_column(importance, var = "variable")

decrease.gini.plot <- ggplot(data = importance, aes(x = reorder(variable, MeanDecreaseGini), y = MeanDecreaseGini, fill = MeanDecreaseGini)) +
  labs(x = "variable", title = "Reducción de pureza (Gini)") +
  geom_col() +
  coord_flip() +
  theme_bw() +
  theme(legend.position = "bottom")

decrease.gini.plot
```

# Preprocesamiento

## Particionado de datos
Particionamos los datos en un 0.8 para entrenamiento y un 0.2 para pruebas.
```{r Particionado de datos}
train <- createDataPartition(y = data$target, p = 0.8, list = FALSE, times = 1)

data.train <- data[train,]
data.test <- data[-train,]
```

## Recipientes y cambio de valores
Creamos un recipiente para cualquier transformación a nuevas variables agregadas y formatos, tambien se apliquen.
```{r Recipientes y cambio de valores}
data.recipe <- recipe(formula = target ~ age + sex + cp + trestbps + chol + fbs + restecg + thalach + exang + oldpeak + slope + ca + thal, data = data.train)
```

## Variables con varianzas cero
Tenemos que evaluar la varianza, porque al tener datos con 0 varianza, no tenemos información. No se detecta ningún predictor con varianza cero, entonces se procede a los siguientes pasos.
```{r Variables con varianzas cero}
tmp <- data %>% dplyr::select(age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal) %>% nearZeroVar(saveMetrics = TRUE) 
datatable(tmp) %>% formatPercentage("percentUnique", 2) %>% formatRound("freqRatio", 2)

data.recipe <- data.recipe %>% step_nzv(all_predictors())
```


## Escalado de datos
Escalamos o estandarizamos los datos (transformamos), tanto variables continuas, discretas y variables como factor.
```{r Escalado de datos}
data.recipe <- data.recipe %>% step_center(all_numeric())
data.recipe <- data.recipe %>% step_scale(all_numeric())

trained.recipe <- prep(data.recipe, training = data.train)
trained.recipe

# Aplicando las transformaciones a los dos conjuntos de datos

data.train.recipe <- bake(trained.recipe, new_data = data.train)
data.test.recipe <- bake(trained.recipe, new_data = data.test)

glimpse(data.train.recipe)

data.train.recipe
data.test.recipe
```

## Seleccionar predictores
Seleccionamos las variables predictoras utilizando algoritmos genéticos. Según el análisis seleccionamos las siguientes variables: `age`, `cp`, `chol`, `restecg`, `exang`, `ca`, `thal`.
```{r Seleccionar predictores}
workers<-makeCluster(4)
registerDoParallel(workers)

# Control de entrenamiento
ga.ctrl <- gafsControl(functions = rfGA,
                       method = "cv",
                       number = 5,
                       allowParallel = TRUE,
                       genParallel = TRUE, 
                       verbose = FALSE)

# Selección de predictores
set.seed(10)
rf.ga <- gafs(x = data.train.recipe[, -14],
              y = data.train.recipe$target,
              iters = 10, 
              popSize = 10,
              gafsControl = ga.ctrl,
              ntree = 100)
rf.ga$optVariables

data.variables.predict <- rf.ga$optVariables
```

# Aplicación de models
Lo models que vamos a aplicar son los siguientes:

- **KNN**
- **Naive Bayes**
- **Regresión logística**
- **Análisis de discriminante lineal**
- **Árbol de clasificación simple**
- **RandomForest**
- **Gradient Boosting**
- **SVM**
- **Redes neuronales**

## K-Nearest Kneightbors KNN
```{r Random forest}
workers <- makeCluster(4)
registerDoParallel(workers)

# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
partitions  <- 10
repetitions <- 5

# Hiperparámetros
hyperparameters <- data.frame(k = c(1, 2, 5, 10, 15, 20, 30, 50))

set.seed(123)
seeds <- vector(mode = "list", length = (partitions * repetitions) + 1)
for (i in 1:(partitions * repetitions)) {
  seeds[[i]] <- sample.int(1000, nrow(hyperparameters)) 
}
seeds[[(partitions * repetitions) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
control.train <- trainControl(method = "repeatedcv", number = partitions,
                              repeats = repetitions, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
set.seed(342)
model.knn <- train(target ~ ., data = data.train.recipe,
                    method = "knn",
                    tuneGrid = hyperparameters,
                    metric = "Accuracy",
                    trControl = control.train)

model.knn
```

### Representación gráfica del KNN
```{r Representación gráfica del KNN}
ggplot(model.knn, highlight = TRUE) +
  scale_x_continuous(breaks = hyperparameters$k) +
  labs(title = "Evolución del accuracy del modelo KNN", x = "K") +
  theme_bw()
```

### Extraer predicciones de KKN
```{r Extraer predicciones de KNN}
setwd("/mnt/napster_disk/LEAD University/IQ - 2020/Minería de Datos Avanzada/examen_predictivo/")
data.train.unformated <- read.csv("datos.corazon.test.csv", sep = ",", header = TRUE)
data.train.unformated$sex <- if_else(data.train.unformated$sex == 1, "masculino", "femenino")

data.test.recipe.new <- bake(trained.recipe, new_data = data.train.unformated)

predictions.knn <- extractPrediction(
                  models = list(knn = model.knn),
                  unkX = data.test.recipe[,-14]
                  )
```

## Naive Bayes

```{r Naive Bayes}
workers<-makeCluster(4)
registerDoParallel(workers)

# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
partitions  <- 10
repetitions <- 5

# Hiperparámetros
hyperparameters <- data.frame(usekernel = FALSE, fL = 0 , adjust = 0)

set.seed(123)
seeds <- vector(mode = "list", length = (partitions * repetitions) + 1)
for (i in 1:(partitions * repetitions)) {
  seeds[[i]] <- sample.int(1000, nrow(hyperparameters))
}
seeds[[(partitions * repetitions) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
#===============================================================================
control.train <- trainControl(method = "repeatedcv", number = partitions,
                              repeats = repetitions, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
# ==============================================================================
set.seed(342)
model.nb <- train(target ~ ., data = data.train.recipe,
                   method = "nb",
                   tuneGrid = hyperparameters,
                   metric = "Accuracy",
                   trControl = control.train)

model.nb
```

### Extrayendo predicciones del Naive Bayes

```{r Extrayendo predicciones del Naive Bayes}
predictions.nb <- extractPrediction(
                  models = list(nb = model.nb),
                  unkX = data.test.recipe[-14]
                  )

names(data.test.recipe)
```

## Regresión logística

```{r Regresion logistica}
workers<-makeCluster(4)
registerDoParallel(workers)

# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
partitions  <- 10
repetitions <- 5

# Hiperparámetros
hyperparameters <- data.frame(parameter = "none")

set.seed(123)
seeds <- vector(mode = "list", length = (partitions * repetitions) + 1)
for (i in 1:(partitions * repetitions)) {
  seeds[[i]] <- sample.int(1000, nrow(hyperparameters))
}
seeds[[(partitions * repetitions) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
control.train <- trainControl(method = "repeatedcv", number = partitions,
                              repeats = repetitions, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
set.seed(342)
model.logistic <- train(target ~ ., data = data.train.recipe,
                         method = "glm",
                         tuneGrid = hyperparameters,
                         metric = "Accuracy",
                         trControl = control.train,
                         family = "binomial")

model.logistic
summary(model.logistic$finalModel)
```

### Extraer predicciones de Regresión logística
```{r Extraer predicciones de Regresion Logistica}
predictions.logistic <- extractPrediction(
                  models = list(logistic = model.logistic),
                  unkX = data.test.recipe[,-14]
                  )
```

## Análisis de discriminante lineal
```{r Analisis de discriminante lineal}
workers<-makeCluster(4)
registerDoParallel(workers)

# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
partitions  <- 10
repetitions <- 5

# Hiperparámetros
hyperparameters <- data.frame(parameter = "none")

set.seed(123)
seeds <- vector(mode = "list", length = (partitions * repetitions) + 1)
for (i in 1:(partitions * repetitions)) {
  seeds[[i]] <- sample.int(1000, nrow(hyperparameters))
}
seeds[[(partitions * repetitions) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
control.train <- trainControl(method = "repeatedcv", number = partitions,
                              repeats = repetitions, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
set.seed(342)
model.lda <- train(target ~ ., data = data.train.recipe,
                    method = "lda",
                    tuneGrid = hyperparameters,
                    metric = "Accuracy",
                    trControl = control.train)

model.lda
```

### Extraer predicciones del Análisis de discriminante lineal
```{r Extraer predicciones del analisis de discriminante lineal}
predictions.lda <- extractPrediction(
                  models = list(lda = model.lda),
                  testX = data.test.recipe[,-14],
                  testY = data.test.recipe$target
                  )

```

## Árbol de clasificación simple
```{r Arbol de clasificacion simple}
workers<-makeCluster(4)
registerDoParallel(workers)

# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
partitions  <- 10
repetitions <- 5

# Hiperparámetros
hyperparameters <- data.frame(parameter = "none")

set.seed(123)
seeds <- vector(mode = "list", length = (partitions * repetitions) + 1)
for (i in 1:(partitions * repetitions)) {
  seeds[[i]] <- sample.int(1000, nrow(hyperparameters))
}
seeds[[(partitions * repetitions) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
control.train <- trainControl(method = "repeatedcv", number = partitions,
                              repeats = repetitions, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
set.seed(342)
model.C50Tree <- train(target ~ ., data = data.train.recipe,
                    method = "C5.0Tree",
                    tuneGrid = hyperparameters,
                    metric = "Accuracy",
                    trControl = control.train)
model.C50Tree
```

### Extraer predicciones del Árbol de clasificación simple
```{r Extraer predicciones del arbol de clasificacion simple}
predictions.C50Tree <- extractPrediction(
                  models = list(C50Tree = model.C50Tree),
                  unkX = data.test.recipe[,-14]
                  )
```

## Random Forest

```{r Random Forest}
workers<-makeCluster(4)
registerDoParallel(workers)

# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
partitions  <- 10
repetitions <- 5

# Hiperparámetros
hyperparameters <- expand.grid(mtry = c(3, 4, 5, 7),
                               min.node.size = c(2, 3, 4, 5, 10, 15, 20, 30),
                               splitrule = "gini")

set.seed(123)
seeds <- vector(mode = "list", length = (partitions * repetitions) + 1)
for (i in 1:(partitions * repetitions)) {
  seeds[[i]] <- sample.int(1000, nrow(hiperparametros))
}
seeds[[(partitions * repetitions) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
control.train <- trainControl(method = "repeatedcv", number = partitions,
                              repeats = repetitions, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL model
set.seed(342)
model.rf <- train(target ~ ., data = data.train.recipe,
                   method = "ranger",
                   tuneGrid = hyperparameters,
                   metric = "Accuracy",
                   trControl = control.train,
                   # Número de árboles ajustados
                   num.trees = 500)
model.rf
```

### Representación gráfica del Random forest
```{r Representacion grafica del Random Forest}
ggplot(model.rf, highlight = TRUE) +
  scale_x_continuous(breaks = 1:30) +
  labs(title = "Evolución del accuracy del modelo Random Forest") +
  guides(color = guide_legend(title = "mtry"),
         shape = guide_legend(title = "mtry")) +
  theme_bw()
```



### Extraer predicciones del Random forest
```{r Extraer predicciones del Random Forest}
predictions.rf <- extractPrediction(
                  models = list(rf = modelo.rf),
                  testX = data.test.recipe[,-14],
                  testY = data.test.recipe$target
                  )

```

## Gradient Boosting

```{r Gradient Boosting}
workers<-makeCluster(4)
registerDoParallel(workers)

# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
partitions  <- 10
repetitions <- 5

# Hiperparámetros
hyperparameters <- expand.grid(interaction.depth = c(1, 2),
                               n.trees = c(500, 1000, 2000),
                               shrinkage = c(0.001, 0.01, 0.1),
                               n.minobsinnode = c(2, 5, 15))

set.seed(123)
seeds <- vector(mode = "list", length = (partitions * repetitions) + 1)
for (i in 1:(partitions * repetitions)) {
  seeds[[i]] <- sample.int(1000, nrow(hyperparameters))
}
seeds[[(partitions * repetitions) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
control.train <- trainControl(method = "repeatedcv", number = partitions,
                              repeats = repetitions, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
set.seed(342)
model.boost <- train(target ~ ., data = data.train.recipe,
                   method = "gbm",
                   tuneGrid = hyperparameters,
                   metric = "Accuracy",
                   trControl = control.train,
                   # Número de árboles ajustados
                   distribution = "adaboost",
                   verbose = FALSE)
model.boost
```

### Representación gráfica del Gradient boosting
```{r Representacion grafica del gradient boosting}
ggplot(model.boost, highlight = TRUE) +
  labs(title = "Evolución del accuracy del modelo Gradient Boosting") +
    guides(color = guide_legend(title = "shrinkage"),
           shape = guide_legend(title = "shrinkage")) +
  theme_bw() +
  theme(legend.position = "bottom")
```

### Extraer predicciones del Gradient boosting
```{r Extraer predicciones del gradient boosting}
predictions.boost <- extractPrediction(
                  models = list(boost = model.boost),
                  testX = data.test.recipe[,-14],
                  testY = data.test.recipe
                  )
predictions.boost
```

## Máquinas de soporte vectorial SVM
```{r Maquinas de soporte vectorial SVM}
workers<-makeCluster(4)
registerDoParallel(workers)

# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
partitions  <- 10
repetitions <- 5

# Hiperparámetros
hyperparameters <- expand.grid(sigma = c(0.001, 0.01, 0.1, 0.5, 1),
                               C = c(1 , 20, 50, 100, 200, 500, 700))

set.seed(123)
seeds <- vector(mode = "list", length = (partitions * repetitions) + 1)
for (i in 1:(partitions * repetitions)) {
  seeds[[i]] <- sample.int(1000, nrow(hyperparameters))
}
seeds[[(partitions * repetitions) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
control.train <- trainControl(method = "repeatedcv", number = partitions,
                              repeats = repetitions, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
set.seed(342)
model.svmrad <- train(target ~ ., data = data.train.recipe,
                   method = "svmRadial",
                   tuneGrid = hyperparameters,
                   metric = "Accuracy",
                   trControl = control.train)
model.svmrad
```

### Representación gráfica del SVM
```{r Representacion grafica del SVM}
ggplot(model.svmrad, highlight = TRUE) +
  labs(title = "Evolución del accuracy del modelo SVM Radial") +
  theme_bw()
```

### Extraer predicciones del SVM
```{r Extraer predicciones del SVM}
predictions.svmrad <- extractPrediction(
                  models = list(svmrad = model.svmrad),
                  testX = data.test.recipe[,-14],
                  testY = data.test.recipe$target
                  )
predictions.svmrad
```

##  Redes Neuronales
```{r Redes neuronales}
workers<-makeCluster(4)
registerDoParallel(workers)

# HIPERPARÁMETROS, NÚMERO DE REPETICIONES Y SEMILLAS PARA CADA REPETICIÓN
partitions  <- 10
repetitions <- 5

# Hiperparámetros
hyperparameters <- expand.grid(size = c(10, 20, 50, 80, 100, 120),
                               decay = c(0.0001, 0.1, 0.5))

set.seed(123)
seeds <- vector(mode = "list", length = (partitions * repetitions) + 1)
for (i in 1:(partitions * repetitions)) {
  seeds[[i]] <- sample.int(1000, nrow(hyperparameters))
}
seeds[[(partitions * repetitions) + 1]] <- sample.int(1000, 1)

# DEFINICIÓN DEL ENTRENAMIENTO
control.train <- trainControl(method = "repeatedcv", number = partitions,
                              repeats = repetitions, seeds = seeds,
                              returnResamp = "final", verboseIter = FALSE,
                              allowParallel = TRUE)

# AJUSTE DEL MODELO
set.seed(342)
model.nnet <- train(target ~ ., data = data.train.recipe,
                   method = "nnet",
                   tuneGrid = hyperparameters,
                   metric = "Accuracy",
                   trControl = control.train,
                   # Rango de inicialización de los pesos
                   rang = c(-0.7, 0.7),
                   # Número máximo de pesos
                   # se aumenta para poder incluir más meuronas
                   MaxNWts = 2000,
                   # Para que no se muestre cada iteración por pantalla
                   trace = FALSE)
model.nnet
```

### Representación gráfica de las NNET
```{r Representacion grafica de las NNET}
ggplot(model.nnet, highlight = TRUE) +
  labs(title = "Evolución del accuracy del modelo NNET") +
  theme_bw()
```

### Extraer predicciones de las NNET
```{r Extraer prediccionesde las NNET}
predictions.nnet <- extractPrediction(
                  models = list(svmrad = model.nnet),
                  testX = data.test.recipe[,-14],
                  testY = data.test.recipe$Survived
                  )
```

# Comparar modelos
```{r Comparar modelos}
models <- list(KNN = model.knn, NB = model.nb, logistic = model.logistic,
                LDA = model.lda, arbol = model.C50Tree, rf = model.rf,
                boosting = model.boost, SVMradial = model.svmrad,
                NNET = model.nnet)

results.resamples <- resamples(modelss)
DT::datatable(resuls.resamples$values)

metrics.resamples <- results.resamples$values %>%
                         gather(key = "modelo", value = "valor", -Resample) %>%
                         separate(col = "modelo", into = c("modelo", "metrica"),
                                  sep = "~", remove = TRUE)
DT::datatable(metrics.resamples)

```

## Comparar el Accuracy con Kappa y promedio
```{r Comparar el Accuracy y kappa promedio}
metrics.resamples %>%
  filter(metrica == "Accuracy") %>%
  group_by(modelo) %>% 
  summarise(media = mean(valor)) %>%
  ggplot(aes(x = reorder(modelo, media), y = media, label = round(media, 2))) +
    geom_segment(aes(x = reorder(modelo, media), y = 0,
                     xend = modelo, yend = media),
                     color = "grey50") +
    geom_point(size = 7, color = "firebrick") +
    geom_text(color = "white", size = 2.5) +
    scale_y_continuous(limits = c(0, 1)) +
    # Accuracy basal
    geom_hline(yintercept = 0.62, linetype = "dashed") +
    annotate(geom = "text", y = 0.72, x = 8.5, label = "Accuracy basal") +
    labs(title = "Validación: Accuracy medio repeated-CV",
         subtitle = "Modelos ordenados por media",
         x = "modelo") +
    coord_flip() +
    theme_bw()
```

# Error de test
```{r Error de test}
predictions <- extractPrediction(
                  models = models,
                  testX = data.test.recipe[, -14],
                  testY = data.test.recipe$target
                  )

datatable(predictions)

metrics.predictions <- predictions %>%
                         mutate(acierto = ifelse(obs == pred, TRUE, FALSE)) %>%
                         group_by(object, dataType) %>%
                         summarise(accuracy = mean(acierto))

DT::datatable(metrics.predictions %>%
  spread(key = dataType, value = accuracy) %>%
  arrange(desc(Test)))
```

## Visualización del error de test
```{r Visualizacion del error de test}
ggplot(data = metrics.predictions,
       aes(x = reorder(object, accuracy), y = accuracy,
           color = dataType, label = round(accuracy, 2))) +
  geom_point(size = 8) +
  scale_color_manual(values = c("orangered2", "gray50")) +
  geom_text(color = "white", size = 3) +
  scale_y_continuous(limits = c(0, 1)) +
  # Accuracy basal
  geom_hline(yintercept = 0.62, linetype = "dashed") +
  annotate(geom = "text", y = 0.66, x = 8.5, label = "Accuracy basal") +
  coord_flip() +
  labs(title = "Accuracy de entrenamiento y test", 
       x = "modelo") +
  theme_bw() + 
  theme(legend.position = "bottom")
```

