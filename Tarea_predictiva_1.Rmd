---
title: "Tarea predictiva 1 para el conjunto de datos PimaIndiansDiabetes2"
date: "`r Sys.Date()`"
author: "Jorge Emilio Zapata Godoy"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, echo=FALSE, cache=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

# Librerias
```{r Cargando las librerias}
library(DT)
library(mlbench)
library(pastecs)
library(stats)
library(ggplot2)
library(gridExtra)
library(plotly)
library(dplyr)
library(tidyverse)
library(boot)
library(Hmisc)
library(ggstatsplot)
library(caret)
library(randomForest)
library(e1071)
library(rpart)
library(reshape2)
library(factoextra)
library(FactoMineR)
library(corrplot)
library(MASS)
library(gbm)
library(xgboost)
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

# Cargando el conjunto de datos

```{r Cargando conjunto de datos}
setwd("/mnt/napster_disk/LEAD University/IQ - 2020/Minería de Datos Avanzada/")
data <- read.csv(file = "diabetes_prediction.csv", header = TRUE, sep = ",", dec = ".")
data <- data.frame(data)
attach(data)
```


# Análisis exploratorio inicial

## Varianzas
Vemos una desviación estándar importante en la variable **glucosa** y la **insulina** de 30.53 y de 118.77, eventualmente evaluaremos la cantidad de outliers y si nos sirven para el modelo de predicción de la diabetes.
```{r Analisis inicial}
DT::datatable(stat.desc(data))
```

## Valores nulos
Observamos una cantidad importante de valores nulos en los valores de la variable *insulin* 
```{r Valores nulos}
NaValue = function (x) {sum(is.na(x))}
DT::datatable(data.frame(apply(data, 2, NaValue)))
```

## Verificando outliers

### Histogramas
La glucosa se presenta como una distribución relativamente uniforme con relación al resto, igual con "pressure". Algunos valores atípicos alejados de la media en la variable "mass" que comprobaremos con boxplots mas adelante.
```{r Histogramas}
# atributes = c("insulin", "triceps", "pressure", "mass", "glucose", "pregnant", "pedigree", "age", "diabetes")
# plot_lst <- vector("list", length = 9)
# 
# for (i in atributes) {
#   histograms <- ggplot(data = data, aes(data[i])) + geom_histogram(breaks=seq(20, 50, by=2)) + labs(title = "Histogram", x = data[i], y = "count")
#   plot_lst <- histograms
# }
# 
# cowplot::plot_grid(plotlist = plot_lst, nrow = 4)
# Por alguna razon esta parte no funciono, pero dejo el codigo por si desea travesearlo

p1 <- qplot(data$pregnant, geom = "histogram", main = "Pregnant", binwidth = 0.5)
p2 <- qplot(data$glucose, geom = "histogram", main = "Glucose", binwidth = 0.5)
p3 <- qplot(data$pressure, geom = "histogram", main = "Pressure", binwidth = 0.5)
p4 <- qplot(data$triceps, geom = "histogram", main = "Triceps", binwidth = 0.5)
p5 <- qplot(data$insulin, geom = "histogram", main = "Insulin", binwidth = 0.5)
p6 <- qplot(data$mass, geom = "histogram", main = "Mass", binwidth = 0.5)
p7 <- qplot(data$pedigree, geom = "histogram", main = "Pedigree", binwidth = 0.5)
p8 <- qplot(data$age, geom = "histogram", main = "Age", binwidth = 0.5)

multiplot(p1, p2, p3, p4, p5, p6, p7, p8, cols = 2)
```

### Box plots
Lo que queremos predecir es la diabetes, es un tipo de variable booleana, por lo tanto podremos hacer boxplots a las variables que vemos outliers en los histogramas para verificar si vale la pena trabajar con ellos.

Observamos muchos datos atipicos en la variable de **insulina**, evaluaremos el modelo con ellos y luego haremos pruebas sin ellos para verificar la predicción.
```{r Boxplots}
insulin_boxplot <- plot_ly(data = data, type = "box", name = "Insulin") %>% add_boxplot(x = ~diabetes, y = ~insulin)
mass_boxplot <- plot_ly(data = data, type = "box", name = "Mass") %>% add_boxplot(x = ~diabetes, y = ~mass)
triceps_boxplots <- plot_ly(data = data, type  = "box", name = "Triceps") %>% add_boxplot(x = ~diabetes, y = ~triceps)
  
#subplot(insulin_boxplot, mass_boxplot, triceps_boxplots, shareY = TRUE) %>% layout(title = "General boxplot")
# Este codigo tampoco compila, aunque al inicio si lo hizo

insulin_boxplot
mass_boxplot
triceps_boxplots
```

## Imputación y transformación

### Bootstrap
Al realizar la prueba de imputación, vemos con 1000 iteraciones una buena distribución del histograma.
```{r Bootstrap}
foo <- function(data, index){
  val <- data[index,]
  fit <- lm(data, data = val)
  return(summary(fit)$r.square)
}

bootstrap <- boot(data = data, statistic = foo, R = 1000)
boot.ci(bootstrap, type = "bca")
plot(bootstrap)
```

### Imputación de datos
Basado en el gráfico anterior haremos imputación a los datos nulos con bootstrap y la media predictiva.

Se realizó completamente la imputación de los datos por medio de bootstrap y evaluaremos los modelos para ver su comportamiento.
```{r Imputacion}
impute_data <- aregImpute(~ insulin + triceps + pressure + mass + glucose + pregnant + pedigree + age + diabetes, data = data, n.impute = 10)

data_set1 <- impute.transcan(impute_data, imputation = 1, data = data, list.out = TRUE, pr = FALSE, check = FALSE)
data_set1 <- data.frame(data_set1)
DT::datatable(data_set1)
```

## Normalización
Empezaremos por aplicar normalización a todos los datos, si esta técnica no funciona, tendremos que quitar los datos atípicos para poder trabajar sin ellos en un mejor modelo predictivo.

### Eliminar outliers en triceps
Antes de realizar la transformación, haremos una eliminación de valores atipicos para verificar que la transformación sea completamente limpia y suavizarlos.
```{r Eliminar outliers en triceps}
Q <- quantile(data_set1$triceps, probs = c(0.25, 0.75), na.rm = TRUE)
iqr <- IQR(data_set1$triceps, na.rm = TRUE)

up_whisker <- min(max(data_set1$triceps), Q[2] + 1.5 * iqr)
down_whisker <- max(min(data_set1$triceps), Q[1] - 1.5 * iqr)

data_set2 <- subset(data_set1, data_set1$triceps > down_whisker)
data_set2 <- subset(data_set1, data_set1$triceps < up_whisker)
nrow(data_set2)
```

### Eliminar outliers en insulin
```{r Eliminar outliers en insulin}
Q <- quantile(data_set1$insulin, probs = c(0.25, 0.75), na.rm = TRUE)
iqr <- IQR(data_set1$insulin, na.rm = TRUE)

up_whisker <- min(max(data_set1$insulin), Q[2] + 1.5 * iqr)
down_whisker <- max(min(data_set1$insulin), Q[1] - 1.5 * iqr)

data_set2 <- subset(data_set1, data_set1$insulin > down_whisker)

data_set2 <- subset(data_set1, data_set1$insulin < up_whisker)

nrow(data_set2)
```

# Cross Validation

## Kfolds
Seleccionamos 9 conjuntos de datos para entrenamiento y 1 para pruebas a partir de la libreria **caret**.
```{r Kfolds}
kfolds <- createFolds(data_set2$diabetes, k = 10)
class(kfolds)
data_set2 <- na.omit(data_set2) # Por si se colo algun valor nulo

data_set2[,c(1:8)] <- scale(data_set2[,c(1:8)])
data_set2
```

## Declarando algoritmos
Declaramos los principales algoritmos para hacer la predicción que se encuentran en la lista, se aplica regresión logística porque estamos trabajando con datos a predecir de tipo booleanos.

- Regresión Logística
- Vecinos mas cercanos (KNN)
- Suport Vector Machines
- Redes Bayesianas
- Árboles de decisión
- Árboles aleatorios (Random Forest)
- Gradient Boosting

### Regresión logística
```{r Regresion logistica}

KfoldsRegresionLogistica <- lapply(kfolds, function(x){
  training.fold <- data_set2[-x, ]
  test.fold <- data_set2[x, ]
  clasificador <- glm(diabetes ~ ., family = binomial, data = training.fold)
  y.pred <- predict(clasificador, type = 'response', newdata = test.fold)
  y.pred <- ifelse(y.pred > 0.5, 1, 0)
  y.pred <- factor(y.pred, levels = c("0", "1"), labels = c("Tiene", "No tiene"))
  cm <- table(test.fold$diabetes, y.pred)
  precision <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] +cm[1,2] + cm[2,1])
  return(precision)
})
```


### KNN
```{r KNN}
KfoldskNN <- lapply(kfolds, function(x){
  training.fold <- data_set2[-x, ]
  test.fold <- data_set2[x, ]
  y.pred <- class::knn(training.fold[ , -9], test.fold[ , -9], cl = training.fold[,9], k = 5)
  cm <- table(test.fold$diabetes, y.pred)
  precision <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] +cm[1,2] + cm[2,1])
  return(precision)
})
```

### Support Vector Machine (SVM)
```{r Support vector machine}
KfoldsKernelSVM <- lapply(kfolds, function(x){
  training.fold <- data_set2[-x, ]
  test.fold <- data_set2[x, ]
  clasificador <- svm(diabetes ~ ., data = training.fold, type = 'C-classification', kernel = 'radial')
  y.pred <- predict(clasificador, newdata = test.fold)
  cm <- table(test.fold$diabetes, y.pred)
  precision <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] +cm[1,2] + cm[2,1])
  return(precision)
})
```


### Redes bayesianas

```{r Redes bayesianas}
KfoldsNaiveBayes <- lapply(kfolds, function(x){
  training.fold <- data_set2[-x, ]
  test.fold <- data_set2[x, ]
  clasificador <- naiveBayes(diabetes ~ ., data = training.fold)
  y.pred <- predict(clasificador, newdata = test.fold)
  cm <- table(test.fold$diabetes, y.pred)
  precision <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] +cm[1,2] + cm[2,1])
  return(precision)
})
```

### Arboles de decisión

```{r Arboles de decision}
KfoldsDecisionTree <- lapply(kfolds, function(x){
  training.fold <- data_set2[-x, ]
  test.fold <- data_set2[x, ]
  clasificador <- rpart(diabetes ~ ., data = training.fold)
  y.pred <- predict(clasificador, newdata = test.fold, type = 'class')
  cm <- table(test.fold$diabetes, y.pred)
  precision <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] +cm[1,2] + cm[2,1])
  return(precision)
})
```

### Random Forest

```{r Random Forest}
kfoldsRandomForest <- lapply(kfolds, function(x){
  training.fold <- data_set2[-x, ]
  test.fold <- data_set2[x, ]
  clasificador <- randomForest(diabetes ~ ., data = training.fold, ntree = 250)
  y.pred <- predict(clasificador, newdata = test.fold)
  cm <- table(test.fold$diabetes, y.pred)
  precision <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] +cm[1,2] + cm[2,1])
  return(precision)
})
```


### Gradient boosting

```{r Gradient Boosting}
kfoldsGradientBoosting <- lapply(kfolds, function(x){
  training.fold <- data_set2[-x,]
  test.fold <- data_set2[x,]
  clasificador <- gbm(diabetes ~ ., data = training.fold, distribution = "gaussian", n.trees = 100, interaction.depth = 4, shrinkage = 0.1)
  y.pred <- predict(clasificador, newdata = test.fold)
  cm <- table(test.fold$diabetes, y.pred)
  precision <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] +cm[1,2] + cm[2,1])
  return(precision)
})
```

## Resultados

```{r Resultados}
# Sacar de la lista los resultados
vKfoldsRegresionLogistica <- unlist(KfoldsRegresionLogistica)
vKfoldskNN <- unlist(KfoldskNN)
vKfoldsKernelSVM<- unlist(KfoldsKernelSVM)
vKfoldsNaiveBayes <- unlist(KfoldsNaiveBayes)
vKfoldsDecisionTree <- unlist(KfoldsDecisionTree)
vkfoldsRandomForest <- unlist(kfoldsRandomForest)
# vkfoldsGradientBoosting <- unlist(kfoldsGradientBoosting)

# Convertir los resultados en un dataframe
df.resultados <- as.data.frame(rbind(
  RegresionLogistica = vKfoldsRegresionLogistica,
  KNN = vKfoldskNN,
  SVM = vKfoldsKernelSVM,
  NaiveBayes = vKfoldsNaiveBayes,
  DecisionTree = vKfoldsDecisionTree,
  RandomForest = vkfoldsRandomForest
  # GradientBoosting = vkfoldsGradientBoosting
))
```

## Visualizando resultados
Extraemos la información de los algoritmos
```{r Resultados}
Medias.Precision <- apply(df.resultados, 1, mean)

df.resultados <- cbind(df.resultados, Medias.Precision)

df.resultados.grafica <- cbind(Metodo = row.names(df.resultados), df.resultados)

df.resultados.grafica <- melt(df.resultados.grafica, id=(c("Metodo")))

colnames(df.resultados.grafica)[2] <- "Fold"
```

### Tabla de resultados
El algoritmo ganador es la regresión logistica con un 0.77% de precisión.
```{r Tabla de resultados}
DT::datatable(df.resultados)
```

### Gráfico de resultados
Observamos una tendencia bastante fija y distribuída por todos los algoritmos.
```{r Grafico de resultados}
graph_results <- ggplot(df.resultados.grafica, aes(x=Fold, y=value, group=Metodo)) +
     geom_line(aes(color=Metodo))+
     geom_point(aes(color=Metodo))
ggplotly(graph_results)
```


